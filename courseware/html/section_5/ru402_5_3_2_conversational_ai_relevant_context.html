<p>
    Current LLM services do not store any conversation history. So, conversations are stateless, which is the same. This means that once a question is asked and the answer generated, we cannot refer to previous passages in the conversation. Keeping the context of the conversation in memory and providing the LLM with the entire conversation (as a list of pairs question + response) together with the new question is the responsibility of the client application.
</p>

> Review the body of the OpenAI <a href="https://platform.openai.com/docs/api-reference/chat">chat completion API</a>, which accepts <code>messages</code>: the list of messages comprising the ongoing conversation.

However, sending back to the LLM the entire conversation may not be convenient for two main reasons.

<p>
    First, we should filter out <b>irrelevant interactions</b> from the current conversation when these do not relate to the last question. So, in practice, imagine a conversation about food interrupted by a few questions about coding and then additional questions about the former food context. Storing all the questions and responses and their corresponding vector embeddings in the user's session enables VSS to find those semantically similar interactions to the last question. Using this method, we can pick the relevant portion of the conversation.
</p>

<div align="center">
    <img width="600px" alt="Retrieval Augmented Generation with Redis" src="../../images/section_5/ru402_5_3_1_conversational_ai_relevant_context.png"/>
</div>

<p>
    The second reason that motivates smart conversation history management is <b>cost reduction</b>. LLM-as-a-service models charge the user based on the number of tokens in the question and the answer. This means that the longer the context, the more expensive the LLM service.
</p>

<p>The idea behind the <b>LLM Conversation Memory</b> is to improve the model quality and personalization through an adaptive memory.
<ul>
    <li>Persist all conversation history (memories) as embeddings in a vector database.</li>
    <li>A conversational agent checks for relevant memories to aid or personalize the LLM behavior.</li>
    <li>Allows users to change topics without misunderstandings seamlessly.</li>
</ul>
</p>


