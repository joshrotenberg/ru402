<p>    
    Vectors are used to represent unstructured data and are essentially lists of decimal numbers. When vectors are used in the context of semantic search, we refer to <b> vector embeddings</b>. 

    The term "embedding" helps reflect the transformation of data to a lower-dimensional space while preserving the features of the original data, so the vector <i>embeds</i> the structures and relationships of the original data (which may be the semantic meaning of a paragraph, or the attributes like colors, angles, lines and shapes in a picture, and so on).
</p>

<div align="center">
<img width="300px" src="../../images/section_1/ru402_1_3_0_embeddings.png" alt="Vectors embed unstructured data features">
</div>
<blockquote>
    Embedding models translate the human-perceived semantic similarity to the vector space
</blockquote>

<p>
    When the original unstructured data is transformed into vector embeddings (or simply "embeddings"), these have variable dimension depending on the chosen method/algorithm to perform the transformation. Different approaches capture the semantics of data in different ways, so the size of the output follows. However, when working with unstructured data, the size of the embeddings must be determined at design time: this is required to be able to perform comparisons. Comparisons between vectors happen in the same vector space, where the vectors have same size.
</p>

<h2>How to create vector embeddings?</h2>
<hr>
<p>
    Embeddings can be generated using several and heterogeneous techniques which facilitate the translation of semantic similarity into similarity between vectors. For texts, we have considered the frequency model, but any method aiming at representing data in the form of a vector may be a valid approach. Nonetheless, besides the quality of the embeddings produced by the chosen method, other criteria must be introduced to validate the viability of the approach. Methods requiring manual intervention do not usually scale well, and may require human expertise and domain knowledge to produce a quality output. For this and other reasons, deep learning techniques and neural networks have become first-class citizens to address the problem and resolve efficiently the data modeling problem. 
</p>

<p>
    The availability of pre-trained machine learning models has helped spread and standardize the approach. In practice, many machine learning projects use pre-trained embedding models as a starting point and through benchmarking and fine-tuning them for the specific task at hand, helps to introduce semantic search into a service.
</p>

<p>
    When vectors are generated by a machine learning model, they embed the distinctive features of data into floating point numbers of fixed size in a compact and dense representation and <i>translate the human-perceived semantic similarity to the vector space</i>. The semantic similarity of two objects (two texts expressing the same concepts and overall meaning, or two similar pictures) translates to the "numerical similarity" of two vectors, which is calculated as the <b>distance between vectors</b>, a simple mathematical operation. 
</p>

<h2>Using embedding models</h2>
<hr>
<p>
The generation of vector embeddings can be automated using commercial or open pre-trained transformer models. The following example creates a 384 dimensions vector of floats, the code sample uses the open <code>all-MiniLM-L6-v2</code> model. To run this example, you can install the Python library <code>sentence_transformers</code> as follows:
</p>

<pre>pip install sentence_transformers</pre>

And then run this code. The first time it is executed, the request model <code>all-MiniLM-L6-v2</code> is downloaded and stored.

<pre>
from sentence_transformers import SentenceTransformer


model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
text = "This is a technical document, it describes the SID sound chip of the Commodore 64" embedding = model.encode(text)

print(embedding[:10])
</pre>

<p>
Different types of unstructured data require specific embedding models to perform the conversion to vector, so you will find models for images, audio files and text, each specialized for the kind of features that must be extracted. Once the model has been evaluated and employed in a project, it is possible to map a dataset to the vector space and work with it. Once each element in your dataset has a vector associated, you can evaluate the similarity between pairs of vectors and determine the semantic similarity of the related objects they represent using a standard method: Vector Similarity Search, which we'll introduce in the next section.
</p>