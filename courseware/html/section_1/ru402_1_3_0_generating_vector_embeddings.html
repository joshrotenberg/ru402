<p>    
    Vectors represent unstructured data and are essentially lists of decimal numbers. When vectors are used in semantic search, we refer to <b> vector embeddings</b>. 

    The term "embedding" helps reflect the transformation of data to a lower-dimensional space while preserving the features of the original data, so the vector <i>embeds</i> the structures and relationships of the original data (which may be the semantic meaning of a paragraph or the attributes like colors, angles, lines, and shapes in a picture, and so on).
</p>

<div align="center">
<img width="300px" src="../../images/section_1/ru402_1_3_0_embeddings.png" alt="Vectors embed unstructured data features">
</div>
<blockquote>
    Embedding models translate the human-perceived semantic similarity to the vector space
</blockquote>

<p>
    When the original unstructured data is transformed into vector embeddings (or simply "embeddings"), these have variable dimensions depending on the chosen method/algorithm to perform the transformation. Different approaches capture data semantics differently, so the output size follows. However, when working with unstructured data, the size of the embeddings must be determined at design time; this is required to be able to perform comparisons. Comparisons between vectors happen in the same vector space, where the vectors have the same size.
</p>

<h2>How to create vector embeddings?</h2>
<hr>
<p>
    Embeddings can be generated using several heterogeneous techniques that facilitate the translation of semantic similarity into similarity between vectors. We have considered the frequency model for texts, but any method aiming at representing data in the form of a vector may be a valid approach. Nonetheless, other aspects should be considered to validate the approach's viability besides the quality of the embeddings. Procedures requiring manual intervention usually scale poorly and may require human expertise and domain knowledge to produce quality output. For this and other reasons, deep learning techniques and neural networks have become first-class citizens in addressing the problem and resolve efficiently the data modeling problem. 
</p>

<p>
    The availability of pre-trained machine learning models has helped spread and standardize the approach. In practice, many machine learning projects use pre-trained embedding models as a starting point, and benchmarking and fine-tuning them for the specific task helps introduce semantic search into a service.
</p>

<p>
    When a machine learning model generates vectors, they embed the distinctive features of data into floating point numbers of fixed size in a compact and dense representation and <i>translate the human-perceived semantic similarity to the vector space</i>. The semantic similarity of two objects (two texts expressing the same concepts and overall meaning or two similar pictures) translates to the "numerical similarity" of two vectors, which is calculated as the <b>distance between vectors</b>, a simple mathematical operation. 
</p>

<h2>Using embedding models</h2>
<hr>
<p>
The generation of vector embeddings can be automated using commercial or open pre-trained transformer models. 
</p>

<h2>Working with text</h2>
<hr>

<p>
The following example creates a 384-dimension vector of floats. The code sample uses the open <code>all-MiniLM-L6-v2</code> model. To run this example, you can install the Python library <code>sentence_transformers</code> as follows:
</p>

<pre>pip install sentence_transformers</pre>

And then run this code. The first time it is executed, the request model <code>all-MiniLM-L6-v2</code> is downloaded and stored.

<pre><code>
from sentence_transformers import SentenceTransformer


model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
text = "This is a technical document, it describes the SID sound chip of the Commodore 64" embedding = model.encode(text)

print(embedding[:10])
</code></pre>

<h2>Working with images</h2>
<hr>

<p>
    You can think of various applications that, based on the similarity of images, implement use cases like product recommendation based on the aspect, object recognition, face recognition, and more. You can convert images to vector embeddings using commercial or free pre-trained models.
</p>

<p>
    As an example, you may use <a href="https://pypi.org/project/imgbeddings/">imgbeddings</a>. This library produces 768-dimension vectors of floats that can be used for image similarity, among other uses. Install the library in a Python virtual environment:
</p>

<pre>pip install imgbeddings</pre>

And run the following code to print on the screen the first five elements of the vector corresponding to an image downloaded from the Internet.

<pre><code>
import requests
from PIL import Image
from imgbeddings import imgbeddings


url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
ibed = imgbeddings()
embedding = ibed.to_embeddings(image)
print(embedding[0][0:5])
</code></pre>

<h2>Summary</h2>
<hr>

<p>
Different types of unstructured data require specific embedding models to perform the conversion to vector, so you will find models for images, audio files, and text, each specialized for the features that must be extracted. Once the model has been evaluated and employed in a project, it is possible to map a dataset to the vector space and work with it. Once each element in your dataset has a vector associated, you can evaluate the similarity between pairs of vectors and determine the semantic similarity of the related objects they represent using a standard method: Vector Similarity Search, which we'll introduce in the next section.
</p>