<p>    
    Vectors represent unstructured data and are essentially lists of decimal numbers. When vectors are used in semantic search, we refer to <b> vector embeddings</b>. 

    The term "embedding" helps reflect the transformation of data to a lower-dimensional space while preserving the features of the original data, so the vector <i>embeds</i> the structures and relationships of the original data (which may be the semantic meaning of a paragraph or the attributes like colors, angles, lines, and shapes in a picture, and so on).
</p>

<div align="center">
<img width="300px" src="../../images/section_1/ru402_1_3_0_embeddings.png" alt="Vectors embed unstructured data features">
</div>
<blockquote>
    Embedding models translate the human-perceived semantic similarity to the vector space
</blockquote>

<p>
    When the original unstructured data is transformed into vector embeddings (or simply "embeddings"), these have variable dimensions depending on the chosen method/algorithm to perform the transformation. Different approaches capture data semantics differently, so the output size follows. However, when working with unstructured data, the size of the embeddings must be determined at design time; this is required to be able to perform comparisons. Comparisons between vectors happen in the same vector space, where the vectors have the same size.
</p>

<h2>How to create vector embeddings?</h2>
<hr>
<p>
    Embeddings can be generated using several heterogeneous techniques that facilitate the translation of semantic similarity into similarity between vectors. We have considered the frequency model for texts, but any method aiming at representing data in the form of a vector may be a valid approach. Nonetheless, other aspects should be considered to validate the approach's viability besides the quality of the embeddings. Procedures requiring manual intervention usually scale poorly and may require human expertise and domain knowledge to produce quality output. For this and other reasons, deep learning techniques and neural networks have become first-class citizens in addressing the problem and resolve efficiently the data modeling problem. 
</p>

<p>
    The availability of pre-trained machine learning models has helped spread and standardize the approach. In practice, many machine learning projects use pre-trained embedding models as a starting point, and benchmarking and fine-tuning them for the specific task helps introduce semantic search into a service.
</p>

<p>
    When a machine learning model generates vectors, they embed the distinctive features of data into floating point numbers of fixed size in a compact and dense representation and <i>translate the human-perceived semantic similarity to the vector space</i>. The semantic similarity of two objects (two texts expressing the same concepts and overall meaning or two similar pictures) translates to the "numerical similarity" of two vectors, which is calculated as the <b>distance between vectors</b>, a simple mathematical operation. 
</p>
